{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f214fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re,string,unicodedata\n",
    "from sklearn.metrics import classification_report,confusion_matrix,f1_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Importing Dependencies for Feature Engineering \n",
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd \n",
    "from prettytable import PrettyTable\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5225710",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed3e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv(r'IMDB-Dataset.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22ae8d",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e34dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#customize stopword as per data\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "new_stopwords = [\"la la la\",\"sunday night\", \"friday night\", \"bla bla bla\", \"movie\",\"one\",\"film\",\"would\",\"shall\",\"could\",\"might\"]\n",
    "stop_words.extend(new_stopwords)\n",
    "stop_words.remove(\"not\")\n",
    "stop_words=set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"'Data Cleaning and Preprocessing pipeline'\"\"\n",
    "\n",
    "#Remove Special Characters\n",
    "def remove_special_character(content):\n",
    "    return re.sub('\\[[^@#$&!%*]*\\]', '', content)\n",
    "\n",
    "#Removing URL's\n",
    "def remove_url(content):\n",
    "    return re.sub(r'http\\S+', '', content)\n",
    "\n",
    "#Removing the Stopwords from Content\n",
    "def remove_stopwords(content):\n",
    "    clean_data= []\n",
    "    for i in content.split():\n",
    "        if i.strip().lower() not in stop_words and i.strip().lower().isalpha():\n",
    "            clean_data.append(i.strip().lower())\n",
    "    return \" \".join(clean_data)\n",
    "\n",
    "# Expension of English Characters\n",
    "def contraction_expansion(content):\n",
    "    content = re.sub(r\"won\\'t\", \"would not\", content)\n",
    "    content = re.sub(r\"can\\'t\", \"can not\", content)\n",
    "    content = re.sub(r\"don\\'t\", \"do not\", content)\n",
    "    content = re.sub(r\"shouldn\\'t\", \"should not\", content)\n",
    "    content = re.sub(r\"needn\\'t\", \"need not\", content)\n",
    "    content = re.sub(r\"hasn\\'t\", \"has not\", content)\n",
    "    content = re.sub(r\"haven\\'t\", \"have not\", content)\n",
    "    content = re.sub(r\"werent\\'t\", \"were not\", content)\n",
    "    content = re.sub(r\"mightn\\'t\", \"might not\", content)\n",
    "    content = re.sub(r\"didn\\'t\", \"did not\", content)\n",
    "    content = re.sub(r\"n\\'t\", \" not\", content)\n",
    "    content = re.sub(r\"\\'re\", \"are\", content)\n",
    "    content = re.sub(r\"\\'s\", \"is\", content)\n",
    "    content = re.sub(r\"\\'d\", \"would\", content)\n",
    "    content = re.sub(r\"\\'ll\", \"will\", content)\n",
    "    content = re.sub(r\"\\'ve\", \"have\", content)\n",
    "    content = re.sub(r\"\\'m\", \"am\", content)\n",
    "    return content\n",
    "\n",
    "# Data Preprocessing\n",
    "def data_cleaning(content):\n",
    "    content = remove_special_character(content)\n",
    "    content = remove_url(content)\n",
    "    content = contraction_expansion(content)            #Contraction_expansion() must be run first then run remove_stopwords() \n",
    "    content = remove_stopwords(content)\n",
    "    return content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b96b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Data Cleaning\n",
    "df['Reviews_clean'] = df['Reviews'].apply(data_cleaning)\n",
    "df['Reviews_clean'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ed90f",
   "metadata": {},
   "source": [
    "# Data OverView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkin Missing Values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9aad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ratings'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reviews_clean'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6220d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique Reviews:%s' % df.Reviews_clean.nunique())\n",
    "print('Unique Movies:%s' % df.Movies.nunique())\n",
    "print('No. of Ratings:%s' % df.Ratings.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73dd25a",
   "metadata": {},
   "source": [
    "# Expolatory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Class Imbalancing\n",
    "sns.countplot(x=df[\"Ratings\"])\n",
    "plt.show()\n",
    "print(df[\"Ratings\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f607f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Visualization of Important Words From Positive Review\"\"\"\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "words = ' '.join(map(str, df[\"Reviews_clean\"][df[\"Ratings\"] >= 7])).split()\n",
    "\n",
    "# Create a counter for word frequencies\n",
    "word_counter = Counter(words)\n",
    "\n",
    "# Create a DataFrame from the counter\n",
    "word_df = pd.DataFrame(list(word_counter.items()), columns=['Word', 'Frequency'])\n",
    "\n",
    "# Sort the DataFrame by frequency in descending order\n",
    "word_df = word_df.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Select the top N words to visualize\n",
    "top_words = 30\n",
    "word_df_top = word_df.head(top_words)\n",
    "\n",
    "# Plot the horizontal bar chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Frequency', y='Word', data=word_df_top, palette='viridis')\n",
    "plt.title('Top {} Words in Positive Reviews'.format(top_words))\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b70bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Visualization of Important Words From Negative Review\"\"\"\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming 'neg' contains negative reviews\n",
    "words = ' '.join(map(str, df[\"Reviews_clean\"][df[\"Ratings\"] <= 4])).split()\n",
    "\n",
    "# Create a counter for word frequencies\n",
    "word_counter = Counter(words)\n",
    "\n",
    "# Create a DataFrame from the counter\n",
    "word_df = pd.DataFrame(list(word_counter.items()), columns=['Word', 'Frequency'])\n",
    "\n",
    "# Sort the DataFrame by frequency in descending order\n",
    "word_df = word_df.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Select the top N words to visualize\n",
    "top_words = 30\n",
    "word_df_top = word_df.head(top_words)\n",
    "\n",
    "# Plot the horizontal bar chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Frequency', y='Word', data=word_df_top, palette='viridis')\n",
    "plt.title('Top {} Words in Negative Reviews'.format(top_words))\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330cbb00",
   "metadata": {},
   "source": [
    "# Statistical Review of Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of Number of Character in Reviews\n",
    "figure,(pos_ax,neg_ax)=plt.subplots(1,2,figsize=(15,18))\n",
    "len_pos_review=df[df['Ratings']>=7]['Reviews_clean'].str.len()\n",
    "pos_ax.hist(len_pos_review,color='green')\n",
    "pos_ax.set_title('Positive Reviews')\n",
    "len_neg_review=df[df['Ratings']<=4]['Reviews_clean'].str.len()\n",
    "neg_ax.hist(len_neg_review,color='red')\n",
    "neg_ax.set_title('Negative Reviews')\n",
    "figure.suptitle('Number of Characters in Reviews')\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of Number of Word in Reviews\n",
    "figure,(pos_ax,neg_ax)=plt.subplots(1,2,figsize=(15,18))\n",
    "pos_word=df[df['Ratings']>=7]['Reviews_clean'].str.split().map(lambda review: len(review))\n",
    "pos_ax.hist(pos_word,color='green')\n",
    "pos_ax.set_title('Number of Words in Positive Reviews')\n",
    "neg_word=df[df['Ratings']<=4]['Reviews_clean'].str.split().map(lambda review: len(review))\n",
    "neg_ax.hist(neg_word,color='red')\n",
    "neg_ax.set_title('Number of Words in Negative Reviews')\n",
    "figure.suptitle('Number of Words in Reviews')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7fae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of Words in Reviews \n",
    "figure,(pos_ax,neg_ax)=plt.subplots(1,2,figsize=(15,18))\n",
    "pos_word=df[df['Ratings']>=7]['Reviews_clean'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(pos_word.map(lambda x :np.mean(x)), ax=pos_ax,color='green')\n",
    "pos_ax.set_title('Positive Reviews')\n",
    "neg_word=df[df['Ratings']<=4]['Reviews_clean'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(neg_word.map(lambda x: np.mean(x)), ax=neg_ax,color='red')\n",
    "pos_ax.set_title('Negative Reviews')\n",
    "figure.suptitle('Average Word Length in Reviews')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59179d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important Features by Usin CountVectorizor\n",
    "def get_top_text_ngrams(corpus, n, g):\n",
    "    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_uni = get_top_text_ngrams(df.Reviews_clean[df['Ratings']>=7],20,1)\n",
    "most_common_uni = dict(most_common_uni)\n",
    "temp =pd.DataFrame(columns= [\"Common Words\", 'Count'])\n",
    "temp['Common Words'] = list(most_common_uni.keys())\n",
    "temp['Count'] = list(most_common_uni.values())\n",
    "fig = px.bar(temp, x='Count', y='Common Words', title='Common Words in Positive Reviews', orientation='h', width=700, height= 700,color='Common Words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_uni = get_top_text_ngrams(df.Reviews_clean[df['Ratings']>=7],20,2)\n",
    "most_common_uni = dict(most_common_uni)\n",
    "temp =pd.DataFrame(columns= [\"Common Words\", 'Count'])\n",
    "temp['Common Words'] = list(most_common_uni.keys())\n",
    "temp['Count'] = list(most_common_uni.values())\n",
    "fig = px.bar(temp, x='Count', y='Common Words', title='Common Bigram Words in Positive Reviews', orientation='h', width=700, height= 700,color='Common Words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ce799",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_uni = get_top_text_ngrams(df.Reviews_clean[df['Ratings']>=7],20,3)\n",
    "most_common_uni = dict(most_common_uni)\n",
    "temp =pd.DataFrame(columns= [\"Common Words\", 'Count'])\n",
    "temp['Common Words'] = list(most_common_uni.keys())\n",
    "temp['Count'] = list(most_common_uni.values())\n",
    "fig = px.bar(temp, x='Count', y='Common Words', title='Common Trigram Words in Positive Reviews', orientation='h', width=700, height= 700,color='Common Words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec236a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth=1000\n",
    "df[[\"Reviews_clean\",\"Ratings\",\"Movies\"]][(df['Ratings']>=7) & (df['Reviews_clean'].str.contains('not like| not want| not good| not perfect| not fine| not excellent| not expecting| not saying| not want| not feel| not sure| not much| not even| not know| not really| not get| not think'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b041428",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_uni = get_top_text_ngrams(df.Reviews_clean[df['Ratings']>=7],20,4)\n",
    "most_common_uni = dict(most_common_uni)\n",
    "temp =pd.DataFrame(columns= [\"Common Words\", 'Count'])\n",
    "temp['Common Words'] = list(most_common_uni.keys())\n",
    "temp['Count'] = list(most_common_uni.values())\n",
    "fig = px.bar(temp, x='Count', y='Common Words', title='Common 4-gram Words in Positive Reviews', orientation='h', width=700, height= 700,color='Common Words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_uni = get_top_text_ngrams(df.Reviews_clean[df['Ratings']>=7],20,5)\n",
    "most_common_uni = dict(most_common_uni)\n",
    "temp =pd.DataFrame(columns= [\"Common Words\", 'Count'])\n",
    "temp['Common Words'] = list(most_common_uni.keys())\n",
    "temp['Count'] = list(most_common_uni.values())\n",
    "fig = px.bar(temp, x='Count', y='Common Words', title='Common 5-gram Words in Positive Reviews', orientation='h', width=700, height= 700,color='Common Words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae04703",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth=1000\n",
    "df[[\"Reviews_clean\",\"Ratings\",\"Movies\"]][(df['Ratings']>=7) & (df['Reviews_clean'].str.contains('mario mario mario mario| blah blah blah blah| la la la la'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_uni = get_top_text_ngrams(df.Reviews_clean[df['Ratings']<=4],20,1)\n",
    "most_common_uni = dict(most_common_uni)\n",
    "temp =pd.DataFrame(columns= [\"Common Words\", 'Count'])\n",
    "temp['Common Words'] = list(most_common_uni.keys())\n",
    "temp['Count'] = list(most_common_uni.values())\n",
    "fig = px.bar(temp, x='Count', y='Common Words', title='Common Unigram Words in Negative Reviews', orientation='h', width=700, height= 700,color='Common Words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485eb39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_uni = get_top_text_ngrams(df.Reviews_clean[df['Ratings']<=4],30,2)\n",
    "most_common_uni = dict(most_common_uni)\n",
    "temp =pd.DataFrame(columns= [\"Common Words\", 'Count'])\n",
    "temp['Common Words'] = list(most_common_uni.keys())\n",
    "temp['Count'] = list(most_common_uni.values())\n",
    "fig = px.bar(temp, x='Count', y='Common Words', title='Common Bigram Words in Negative Reviews', orientation='h', width=700, height= 700,color='Common Words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275bb0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_uni = get_top_text_ngrams(df.Reviews_clean[df['Ratings']<=4],40,3)\n",
    "most_common_uni = dict(most_common_uni)\n",
    "temp =pd.DataFrame(columns= [\"Common Words\", 'Count'])\n",
    "temp['Common Words'] = list(most_common_uni.keys())\n",
    "temp['Count'] = list(most_common_uni.values())\n",
    "fig = px.bar(temp, x='Count', y='Common Words', title='Common Trigram Words in Negative Reviews', orientation='h', width=700, height= 700,color='Common Words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_uni = get_top_text_ngrams(df.Reviews_clean[df['Ratings']<=4],45,4)\n",
    "most_common_uni = dict(most_common_uni)\n",
    "temp =pd.DataFrame(columns= [\"Common Words\", 'Count'])\n",
    "temp['Common Words'] = list(most_common_uni.keys())\n",
    "temp['Count'] = list(most_common_uni.values())\n",
    "fig = px.bar(temp, x='Count', y='Common Words', title='Common 4-gram Words in Negative Reviews', orientation='h', width=700, height= 700,color='Common Words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a9e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_uni = get_top_text_ngrams(df.Reviews_clean[df['Ratings']<=4],50,5)\n",
    "most_common_uni = dict(most_common_uni)\n",
    "temp =pd.DataFrame(columns= [\"Common Words\", 'Count'])\n",
    "temp['Common Words'] = list(most_common_uni.keys())\n",
    "temp['Count'] = list(most_common_uni.values())\n",
    "fig = px.bar(temp, x='Count', y='Common Words', title='Common 5-gram Words in Negative Reviews', orientation='h', width=700, height= 700,color='Common Words')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f139016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth=1000\n",
    "df[[\"Reviews_clean\",\"Ratings\",\"Movies\"]][(df['Ratings']<=4) & (df['Reviews_clean'].str.contains('like| good| great'))].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6babc7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth=1000\n",
    "df[[\"Reviews_clean\",\"Ratings\",\"Movies\"]][(df['Ratings']<=4) & (df['Reviews_clean'].str.contains('saturday night friday'))].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d406c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth=1000\n",
    "df[[\"Reviews_clean\",\"Ratings\",\"Movies\"]][(df['Ratings']>=7) & (df['Reviews_clean'].str.contains('saturday night friday'))].head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa891f",
   "metadata": {},
   "source": [
    "# # Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d68222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping the Rating data as 1 is(+ve)(if rating >=7) and 0(-ve)(if Rating<=4) else 2(neutral)(If Rating=5 or 6)\n",
    "df['Label'] = df['Ratings'].apply(lambda x: '1' if x>=7 else('0' if x<=4 else '2'))\n",
    "#Removing\n",
    "df = df[df.Label<'2']\n",
    "data = df[['Reviews_clean','Label']]\n",
    "print(data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Dependencies for Feature Engineering \n",
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd \n",
    "from prettytable import PrettyTable\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68357175",
   "metadata": {},
   "source": [
    "# Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9948612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization of Words\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wordnetlemma = WordNetLemmatizer()\n",
    "    def __call__(self, reviews):\n",
    "        return [self.wordnetlemma.lemmatize(word) for word in word_tokenize(reviews)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a923246",
   "metadata": {},
   "source": [
    "##### Vectorization with Count Vectorizer and TFIDF Vectorizer with Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d7732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(data,test_size=0.3,random_state=42,shuffle=True)\n",
    "countvect = CountVectorizer(analyzer='word', tokenizer= LemmaTokenizer(), ngram_range=(1,1), min_df=10,max_features=500)\n",
    "tfidfvect = TfidfVectorizer(analyzer='word', tokenizer= LemmaTokenizer(), ngram_range=(1,1), min_df=10,max_features=500)\n",
    "x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_count = countvect.fit_transform(test['Reviews_clean']).toarray()\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.fit_transform(test['Reviews_clean']).toarray()\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eedf72",
   "metadata": {},
   "source": [
    "##### Feature Importance with Logistic Regression and CountVectorizer with Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_count,y_train)\n",
    "lgr.score(x_test_count,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable(['Feature','Score'])\n",
    "for feature, importantance in zip(countvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=200:\n",
    "        importantfeature.add_row([feature, importantance])\n",
    "        i=i+1\n",
    "print(importantfeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f54ff2f",
   "metadata": {},
   "source": [
    "###### Feature Importance with Logistic Regression and TFIDF Vectorizer with Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28af7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_tfidf,y_train)\n",
    "lgr.score(x_test_tfidf,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable(['Feature','Score'])\n",
    "for feature, importantance in zip(tfidfvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=100:\n",
    "        importantfeature.add_row([feature, importantance])\n",
    "        i=i+1\n",
    "print(importantfeature)   \n",
    "\n",
    "# We can see that TFIDF give more score to the important words as compared to Countvect example check word \"awful\"--> negative word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb69f2",
   "metadata": {},
   "source": [
    "##### Feature Importance with TFIDF and Count Vectorizer on Bigrams With Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(data,test_size=0.3,random_state=42,shuffle=True)\n",
    "countvect = CountVectorizer(analyzer='word', tokenizer= LemmaTokenizer(), ngram_range=(2,2), min_df=10,max_features=500)\n",
    "tfidfvect = TfidfVectorizer(analyzer='word', tokenizer= LemmaTokenizer(), ngram_range=(2,2), min_df=10,max_features=500)\n",
    "x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_count = countvect.fit_transform(test['Reviews_clean']).toarray()\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.fit_transform(test['Reviews_clean']).toarray()\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ce916",
   "metadata": {},
   "source": [
    "###### Feature Importance with TFIDF  on Bigrams with Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d1b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_tfidf,y_train)\n",
    "lgr.score(x_test_tfidf,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable(['Feature','Score'])\n",
    "for feature, importantance in zip(tfidfvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=100:\n",
    "        importantfeature.add_row([feature, importantance])\n",
    "        i=i+1\n",
    "print(importantfeature)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3eff91",
   "metadata": {},
   "source": [
    "###### Feature Importance with Count Vectorizer  on Bigrams with Logistic Regression\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_count,y_train)\n",
    "lgr.score(x_test_count,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable(['Feature','Score'])\n",
    "for feature, importantance in zip(countvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=200:\n",
    "        importantfeature.add_row([feature, importantance])\n",
    "        i=i+1\n",
    "print(importantfeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729fac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth=1000\n",
    "df[[\"Reviews_clean\",\"Ratings\",\"Movies\"]][(df['Ratings']>=9) & (df['Reviews_clean'].str.contains('bad review'))].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa1380",
   "metadata": {},
   "source": [
    "###### Feature Importance with TFIDF and Count Vectorizer  on Trigrams with Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(data,test_size=0.3,random_state=42,shuffle=True)\n",
    "countvect = CountVectorizer(analyzer='word', tokenizer= LemmaTokenizer(), ngram_range=(3,3), min_df=10,max_features=500)\n",
    "tfidfvect = TfidfVectorizer(analyzer='word', tokenizer= LemmaTokenizer(), ngram_range=(3,3), min_df=10,max_features=500)\n",
    "x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_count = countvect.fit_transform(test['Reviews_clean']).toarray()\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.fit_transform(test['Reviews_clean']).toarray()\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712414ac",
   "metadata": {},
   "source": [
    "###### Feature Importance with Count Vectorizer  on Trigrams with Logistic Regression\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1928cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_count,y_train)\n",
    "lgr.score(x_test_count,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable(['Feature','Score'])\n",
    "for feature, importantance in zip(countvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=100:\n",
    "        importantfeature.add_row([feature, importantance])\n",
    "        i=i+1\n",
    "print(importantfeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dc288",
   "metadata": {},
   "source": [
    "###### Feature Importance with TFIDF Vectorizer  on Trigrams with Logistic Regression\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_tfidf,y_train)\n",
    "lgr.score(x_test_tfidf,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable(['Feature','Score'])\n",
    "for feature, importantance in zip(tfidfvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=100:\n",
    "        importantfeature.add_row([feature, importantance])\n",
    "        i=i+1\n",
    "print(importantfeature)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df262285",
   "metadata": {},
   "source": [
    "###### Feature Importance with TFIDF and Count Vectorizer  on 4-grams with Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "countvect = CountVectorizer(analyzer='word', tokenizer=LemmaTokenizer(), ngram_range=(4, 4), min_df=10, max_features=500)\n",
    "tfidfvect = TfidfVectorizer(analyzer='word', tokenizer=LemmaTokenizer(), ngram_range=(4, 4), min_df=10, max_features=500)\n",
    "\n",
    "x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_count = countvect.transform(test['Reviews_clean']).toarray()  # Use transform instead of fit_transform\n",
    "\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.transform(test['Reviews_clean']).toarray()  # Use transform instead of fit_transform\n",
    "\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a08ce",
   "metadata": {},
   "source": [
    "###### Feature Importance with Count Vectorizer  on 4-grams with Logistic Regression\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_count,y_train)\n",
    "lgr.score(x_test_count,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable(['Feature','Score'])\n",
    "for feature, importantance in zip(countvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=200:\n",
    "        importantfeature.add_row([feature, importantance])\n",
    "        i=i+1\n",
    "print(importantfeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231493f",
   "metadata": {},
   "source": [
    "###### Feature Importance with TFIDF Vectorizer  on 4-grams with Logistic Regression\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_tfidf,y_train)\n",
    "lgr.score(x_test_tfidf,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable(['Feature','Score'])\n",
    "for feature, importantance in zip(tfidfvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=100:\n",
    "        importantfeature.add_row([feature, importantance])\n",
    "        i=i+1\n",
    "print(importantfeature)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d72376",
   "metadata": {},
   "source": [
    "# Vectorization with TFIDF and Count Vectorizer with Unigram, Bigram and Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a206ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data,test_size=.3,random_state=42, shuffle=True)\n",
    "countvect = CountVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3), min_df=10,max_features=5000)\n",
    "tfidfvect = TfidfVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3),min_df=10,max_features=5000)\n",
    "x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_count = countvect.transform(test['Reviews_clean']).toarray()\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.transform(test['Reviews_clean']).toarray()\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8e1d9",
   "metadata": {},
   "source": [
    "### Feature Selection with Chi Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 5000\n",
    "Number = 1\n",
    "featureselection = PrettyTable([\"Unigram\", \"Bigram\",\"Trigram\"])\n",
    "for category in train['Label'].unique():\n",
    "    features_chi2 = chi2(x_train_tfidf, train['Label'] == category)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidfvect.get_feature_names())[indices]\n",
    "    unigrams = [x for x in feature_names if len(x.split(' ')) == 1]\n",
    "    bigrams = [x for x in feature_names if len(x.split(' ')) == 2]\n",
    "    trigrams = [x for x in feature_names if len(x.split(' ')) == 3]\n",
    "    print(\"%s. %s :\" % (Number,category))\n",
    "    print(\"\\t# Unigrams :\\n\\t. %s\" %('\\n\\t. '.join(unigrams[-N:])))\n",
    "    print(\"\\t# Bigrams :\\n\\t. %s\" %('\\n\\t. '.join(bigrams[-N:])))\n",
    "    print(\"\\t# Trigrams :\\n\\t. %s\" %('\\n\\t. '.join(trigrams[-N:])))\n",
    "    Number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d5d382",
   "metadata": {},
   "source": [
    "# Model Selection \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c5238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Prereq Libraries for Model Selection\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf670c93",
   "metadata": {},
   "source": [
    "##### Logistic Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d6b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Pipeline(\n",
    "    steps=[\n",
    "        ('classifier', LogisticRegression())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec953d",
   "metadata": {},
   "source": [
    "##### Training of Logistic Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b937fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_1.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score on Training dataset for Logistic Regression: %s\" % precision_score(y_train, model_1.predict(x_train_tfidf),average='weighted' ))\n",
    "print(\"AUC Score on Training dataset for Logistic Regression: %s\" % roc_auc_score(y_train, model_1.predict(x_train_tfidf),average='weighted' ))\n",
    "print(\"F1 Score on Training dataset for Logistic Regression: %s\" % f1_score(y_train, model_1.predict(x_train_tfidf),average='weighted' ))\n",
    "\n",
    "print(\"Precision Score on Test dataset for Logistic Regression: %s\" % precision_score(y_test, model_1.predict(x_test_tfidf),average='weighted' ))\n",
    "print(\"AUC Score on Test dataset for Logistic Regression: %s\" % roc_auc_score(y_test, model_1.predict(x_test_tfidf),average='weighted' ))\n",
    "print(\"F1 Score on Test dataset for Logistic Regression: %s\" % f1_score(y_test, model_1.predict(x_test_tfidf),average='weighted' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1669c1a",
   "metadata": {},
   "source": [
    "##### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e78c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Pipeline(\n",
    "    steps=[\n",
    "        ('classifier', DecisionTreeClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5ab72",
   "metadata": {},
   "source": [
    "##### Training on Decision Tree  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4734aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_2.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score on Training dataset for DecisionTreeClassifier: %s\" % precision_score(y_train, model_2.predict(x_train_tfidf),average='weighted' ))\n",
    "print(\"AUC Score on Training dataset for DecisionTreeClassifier: %s\" % roc_auc_score(y_train, model_2.predict(x_train_tfidf),average='weighted' ))\n",
    "print(\"F1 Score on Training dataset for DecisionTreeClassifier: %s\" % f1_score(y_train, model_2.predict(x_train_tfidf),average='weighted' ))\n",
    "\n",
    "print(\"Precision Score on Test dataset for DecisionTreeClassifier: %s\" % precision_score(y_test, model_2.predict(x_test_tfidf),average='weighted' ))\n",
    "print(\"AUC Score on Test dataset for DecisionTreeClassifier: %s\" % roc_auc_score(y_test, model_2.predict(x_test_tfidf),average='weighted' ))\n",
    "print(\"F1 Score on Test dataset for Logistic DecisionTreeClassifier: %s\" % f1_score(y_test, model_2.predict(x_test_tfidf),average='weighted' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde2d8d3",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc28896",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Pipeline(\n",
    "    steps=[\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c3dc31",
   "metadata": {},
   "source": [
    "##### Training on Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69354ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_3.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score on Training dataset for RandomForestClassifier: %s\" % precision_score(y_train, model_3.predict(x_train_tfidf),average='weighted' ))\n",
    "print(\"AUC Score on Training dataset for RandomForestClassifier: %s\" % roc_auc_score(y_train, model_3.predict(x_train_tfidf),average='weighted' ))\n",
    "print(\"F1 Score on Training dataset for RandomForestClassifier: %s\" % f1_score(y_train, model_3.predict(x_train_tfidf),average='weighted' ))\n",
    "\n",
    "print(\"Precision Score on Test dataset for RandomForestClassifier: %s\" % precision_score(y_test, model_3.predict(x_test_tfidf),average='weighted' ))\n",
    "print(\"AUC Score on Test dataset for RandomForestClassifier: %s\" % roc_auc_score(y_test, model_3.predict(x_test_tfidf),average='weighted' ))\n",
    "print(\"F1 Score on Test dataset for Logistic RandomForestClassifier: %s\" % f1_score(y_test, model_3.predict(x_test_tfidf),average='weighted' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d248f",
   "metadata": {},
   "source": [
    "##### AdaBoost Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58bb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = Pipeline(\n",
    "    steps=[\n",
    "        ('classifier', AdaBoostClassifier())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b83065",
   "metadata": {},
   "source": [
    "##### Training on Adaboost Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdfb594",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_4.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score on Training dataset for AdaBoostClassifier: %s\" % precision_score(y_train, model_4.predict(x_train_tfidf),average='weighted' ))\n",
    "print(\"AUC Score on Training dataset for AdaBoostClassifier: %s\" % roc_auc_score(y_train, model_4.predict(x_train_tfidf),average='weighted' ))\n",
    "print(\"F1 Score on Training dataset for AdaBoostClassifier: %s\" % f1_score(y_train, model_4.predict(x_train_tfidf),average='weighted' ))\n",
    "\n",
    "print(\"Precision Score on Test dataset for AdaBoostClassifier: %s\" % precision_score(y_test, model_3.predict(x_test_tfidf),average='weighted' ))\n",
    "print(\"AUC Score on Test dataset for AdaBoostClassifier: %s\" % roc_auc_score(y_test, model_4.predict(x_test_tfidf),average='weighted' ))\n",
    "print(\"F1 Score on Test dataset for Logistic AdaBoostClassifier: %s\" % f1_score(y_test, model_4.predict(x_test_tfidf),average='weighted' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c7179",
   "metadata": {},
   "source": [
    "##### Decision Tree with max depth 11 to overcome overfit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceae1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Pipeline(\n",
    "    steps=[\n",
    "        ('classifier', DecisionTreeClassifier(criterion='gini', max_depth=11, min_samples_split=2, min_samples_leaf=1))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c83da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_3.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ee1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score on Training dataset for DecisionTreeClassifier: %s\" % precision_score(y_train, model_2.predict(x_train_tfidf),average='weighted' ))\n",
    "print(\"AUC Score on Training dataset for DecisionTreeClassifier: %s\" % roc_auc_score(y_train, model_2.predict(x_train_tfidf),average='weighted' ))\n",
    "print(\"F1 Score on Training dataset for DecisionTreeClassifier: %s\" % f1_score(y_train, model_2.predict(x_train_tfidf),average='weighted' ))\n",
    "\n",
    "print(\"Precision Score on Test dataset for DecisionTreeClassifier: %s\" % precision_score(y_test, model_2.predict(x_test_tfidf),average='weighted' ))\n",
    "print(\"AUC Score on Test dataset for DecisionTreeClassifier: %s\" % roc_auc_score(y_test, model_2.predict(x_test_tfidf),average='weighted' ))\n",
    "print(\"F1 Score on Test dataset for Logistic DecisionTreeClassifier: %s\" % f1_score(y_test, model_2.predict(x_test_tfidf),average='weighted' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c53e9",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tunning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fd01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "def hyperparamtune(classifier, param_grid, metric, verbose_value,cv):\n",
    "    model = model_selection.GridSearchCV(estimator=classifier,\n",
    "                                        param_grid=param_grid,\n",
    "                                        scoring=metric,\n",
    "                                        verbose=verbose_value,\n",
    "                                        cv=cv)\n",
    "    \n",
    "    model.fit(x_train_tfidf,y_train)\n",
    "    print(\"Best Score %s\" % {model.best_score_})\n",
    "    print(\"Best Hyperparameter set: \")\n",
    "    best_parameters = model.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(f\"\\t{param_name}: {best_parameters[param_name]}\")\n",
    "    return model, best_parameters   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454676f1",
   "metadata": {},
   "source": [
    "##### HyperParameter Tunning of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10aa3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_gd = {\"penalty\":[\"12\",\"11\"],\n",
    "           \"C\":[0.01,0.1,1.0,10],\n",
    "           \"tol\":[0.0001,0.001,0.01],\n",
    "           \"max_iter\":[100,200]}\n",
    "model_7, best_param = hyperparamtune(LogisticRegression(),param_gd,\"accuracy\",10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b1964f",
   "metadata": {},
   "source": [
    "##### Evaluation of FineTuned Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50decec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score on Training dataset for Logistic Regression: %s\" % precision_score(y_train, model_7.predict(x_train_tfidf),average='weighted' ))\n",
    "print(\"AUC Score on Training dataset for Logistic Regression: %s\" % roc_auc_score(y_train, model_7.predict(x_train_tfidf),average='weighted' ))\n",
    "print(\"F1 Score on Training dataset for Logistic Regression: %s\" % f1_score(y_train, model_7.predict(x_train_tfidf),average='weighted' ))\n",
    "\n",
    "print(\"Precision Score on Test dataset for Logistic Regression: %s\" % precision_score(y_test, model_7.predict(x_test_tfidf),average='weighted' ))\n",
    "print(\"AUC Score on Test dataset for Logistic Regression: %s\" % roc_auc_score(y_test, model_7.predict(x_test_tfidf),average='weighted' ))\n",
    "print(\"F1 Score on Test dataset for Logistic Regression: %s\" % f1_score(y_test, model_7.predict(x_test_tfidf),average='weighted' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If System Memory giving space easily then also do this fine tuning on AdaBoost And Decision Tree and other classifier \n",
    "# Fine Tuned Logistic Regression Classifier Giving Best F1 Score....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89003dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data,test_size=.3,random_state=42, shuffle=True)\n",
    "#countvect = CountVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3), min_df=10,max_features=5000)\n",
    "tfidfvect = TfidfVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3),min_df=10,max_features=10000)\n",
    "#x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "#x_test_count = countvect.transform(test['Reviews_clean']).toarray()\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.transform(test['Reviews_clean']).toarray()\n",
    "\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceea5b6",
   "metadata": {},
   "source": [
    "# Model Evaluation Model_1 is without Pipeline and Model_2 is with Pipeline on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c79fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1=LogisticRegression(penalty='l2',dual=False, tol=0.0001, C=10, solver='lbfgs', max_iter=200, multi_class='auto', verbose=0, warm_start=False, n_jobs=None)\n",
    "model_2=Pipeline(\n",
    "    steps=[\n",
    "        #best base model(\"classifier\", LogisticRegression(penalty='l2',dual=False, tol=0.0001, C=1.0, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None)),\n",
    "    ('vect',TfidfVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3),min_df=10,max_features=10000)),(\"classifier\", LogisticRegression(penalty='l2',dual=False, tol=0.0001, C=10, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc64e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_1.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbb0340",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_2.fit(train['Reviews_clean'],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a9d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score for Logistic Regression: %s\" % precision_score(y_test,model_1.predict(x_test_tfidf),average='micro'))\n",
    "print(\"Recall Score for Logistic Regression: %s\" % recall_score(y_test,model_1.predict(x_test_tfidf),average='micro'))\n",
    "print(\"AUC Score for Logistic Regression: %s\" % roc_auc_score(y_test,model_1.predict_proba(x_test_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_1 =f1_score(y_test,model_1.predict(x_test_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score for Logistic Regression: %s\" % f1_score_1)\n",
    "print(\"Accuracy Score for Logistic Regression: %s\" % accuracy_score(y_test,model_1.predict(x_test_tfidf)))\n",
    "print(\"Precision Score for Logistic Regression Pipeline: %s\" % precision_score(y_test,model_2.predict(test['Reviews_clean']),average='micro'))\n",
    "print(\"Recall Score for Logistic Regression Pipeline: %s\" % recall_score(y_test,model_2.predict(test['Reviews_clean']),average='micro'))\n",
    "print(\"AUC Score for Logistic Regression Pipeline: %s\" % roc_auc_score(y_test,model_2.predict_proba(test['Reviews_clean'])[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_2 =f1_score(y_test,model_2.predict(test['Reviews_clean']),average=\"weighted\")\n",
    "print(\"F1 Score for Logistic Regression Pipeline: %s\" % f1_score_2)\n",
    "print(\"Accuracy Score for Logistic Regression Pipeline: %s\" % accuracy_score(y_test,model_2.predict(test['Reviews_clean'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcee434",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=model_1.predict(x_test_tfidf)\n",
    "y_predict_prob=model_1.predict_proba(x_test_tfidf)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9baf48f",
   "metadata": {},
   "source": [
    "# Confusion Matrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_plot(y_test,y_score):\n",
    "    confmatrix = confusion_matrix(y_test,y_score)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(confmatrix)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, confmatrix[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee75451",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_plot(y_test,y_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
